{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyspark-demo.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNQBRjuk4BkXMOCLgA8rcS9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aulia-Rnm/BigData/blob/main/pyspark_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Pengaturan PySpark di Colab\n",
        "\n",
        "  Spark ditulis menggunakan bahasa pemrogrraman Scala dan membutuhkan Java Virtual Machine (JVM) untuk menjalankannya. Sebagai langkah pertama, lakukan instalasi Java dengan menuliskan perintah di bawah ini."
      ],
      "metadata": {
        "id": "mTTYHAfRI6Z2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Untuk instalasi Apache Spark, unduh berkas menggunakan perintah wget kemudian\n",
        "ekstrak dengan perintah tar. Silahkan salin perintah berikut untuk melakukan instalasi."
      ],
      "metadata": {
        "id": "v67HyCuLEtzn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "KRVE5LbBIPBd"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Sebagai Langkah lanjutan, dibutuhkan pengaturan terkait Java dan Spark Home. Untuk melakukannya dapat memanfaatkan script pyton. Silahkan masukkan kode berikut ke dalam notebook."
      ],
      "metadata": {
        "id": "-oPiQIlUE7hR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "lHeat93lE-Yt"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Konfigurasi PySpark\n",
        "\n",
        "  Konfigurasi PySpark dapat dilakukan dengan menginstall library findspark yang digunakan untuk mencari lokasi Spark yang terinstall pada sistem. Proses instalasi dapat memanfaatkan perintah pip, silahkan perhatikan perintah di bawah ini."
      ],
      "metadata": {
        "id": "zrbVBOk7FD1b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Setelah proses instalasi berhasil, lakukan import library dan inisialisasi findspark. Silahkan salin kode berikut ke dalam notebook."
      ],
      "metadata": {
        "id": "DwIJl5yoFQei"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Koneksi ke dalam spark dapat dilakukan memanfaatkan SparkSession. Salin kode berikut, dimana spark menggunakan port 4050."
      ],
      "metadata": {
        "id": "wjzd5SgfFm37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder\\\n",
        " .master(\"local\")\\\n",
        " .appName(\"Colab\")\\\n",
        " .config('spark.ui.port', '4050')\\\n",
        " .getOrCreate()"
      ],
      "metadata": {
        "id": "NrS20Wv0FIH5"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Memasukkan data ke dalam PySpark\n",
        "  \n",
        "  Spark mempunyai beberapa modul untuk membaca data dengan format yang berbeda. Spark secara otomatis akan menentukan tiap tipe data untuk setiap kolom. Data yang akan digunakan sebagai dataset dapat diunduh menggunakan perintah wget. Silahkan perhatikan perintah berikut."
      ],
      "metadata": {
        "id": "po9wil90GahM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --continue https://raw.githubusercontent.com/dhanifudin/pyspark-demo/main/sample_books.json -O /tmp/sample_books.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KbUMLkoGoC5",
        "outputId": "91cd2c98-6413-41b2-d648-0e882a1b72ee"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-06 13:28:17--  https://raw.githubusercontent.com/dhanifudin/pyspark-demo/main/sample_books.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1565 (1.5K) [text/plain]\n",
            "Saving to: ‘/tmp/sample_books.json’\n",
            "\n",
            "\r/tmp/sample_books.j   0%[                    ]       0  --.-KB/s               \r/tmp/sample_books.j 100%[===================>]   1.53K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-06-06 13:28:17 (21.0 MB/s) - ‘/tmp/sample_books.json’ saved [1565/1565]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data yang telah diunduh dapat dibaca dengan menggunakan kode berikut. Silahkan salin kode berikut ke dalam notebook."
      ],
      "metadata": {
        "id": "hcXkFXtPGyKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.json(\"/tmp/sample_books.json\")"
      ],
      "metadata": {
        "id": "t99ZYhuTG2W_"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Analisa data menggunakan PySpark\n",
        "\n",
        "  Sebelum dapat menganalisa dataset, perlu mengetahui schema data yang akan diolah. Schema dapat diketahui dengan menggunakan kode berikut. Kode ini memanfaatkan dataframe."
      ],
      "metadata": {
        "id": "oue5ha49KX47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEXdFX14Kdn0",
        "outputId": "03dbf10b-1e94-441e-df47-726788bd1d5d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- author: string (nullable = true)\n",
            " |-- edition: string (nullable = true)\n",
            " |-- price: double (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- year_written: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Menampilkan data\n",
        "\n",
        "  Dataset dapat ditampilkan dengan perintah show dari dataframe. Perintah show memiliki parameter berupa jumlah record yang ditampilkan serta opsi truncate. Silahkan salin kode beriikut ke dalam notebook."
      ],
      "metadata": {
        "id": "BbaMnTRjKgO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(4,False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlgbF9TOLhY2",
        "outputId": "f3cc2518-01d0-4a01-d780-3a1be251db1b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+--------------+-----+----------------+------------+\n",
            "|author         |edition       |price|title           |year_written|\n",
            "+---------------+--------------+-----+----------------+------------+\n",
            "|Austen, Jane   |Penguin       |18.2 |Northanger Abbey|1814        |\n",
            "|Tolstoy, Leo   |Penguin       |12.7 |War and Peace   |1865        |\n",
            "|Tolstoy, Leo   |Penguin       |13.5 |Anna Karenina   |1875        |\n",
            "|Woolf, Virginia|Harcourt Brace|25.0 |Mrs. Dalloway   |1925        |\n",
            "+---------------+--------------+-----+----------------+------------+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Menghitung jumlah dataset\n",
        "  \n",
        "  Dataset dapat dihitung menggunakan fungsi count, silahkan salin kode berikut ke dalam untuk mengetahui jumlah dataset yang dimiliki."
      ],
      "metadata": {
        "id": "-2f7ONXxNRWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ey-XJyr3NUlK",
        "outputId": "b1374130-9478-4773-b133-d8e19c70dac6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Menampilkan kolom yang diinginkan\n",
        "\n",
        "  Dataset dapat dipilih untuk menampilkan data dengan kolom tertentu dengan fungsi select. Untuk menampilkan hanya kolom title, price dan year_written, silahkan salin kode berikut ke dalam colab."
      ],
      "metadata": {
        "id": "gH0On-RBNXpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(\"title\", \"price\", \"year_written\").show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcBCj9-VNcXL",
        "outputId": "2f935f42-c329-43d1-ab72-bf5030882f3d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-----+------------+\n",
            "|           title|price|year_written|\n",
            "+----------------+-----+------------+\n",
            "|Northanger Abbey| 18.2|        1814|\n",
            "|   War and Peace| 12.7|        1865|\n",
            "|   Anna Karenina| 13.5|        1875|\n",
            "|   Mrs. Dalloway| 25.0|        1925|\n",
            "|       The Hours|12.35|        1999|\n",
            "+----------------+-----+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Me-filter dataset\n",
        "  PySpark dapat juga melakukan filter suatu dataset berdasarkan kondisi yang dibutuhkan. Sebagai contoh: dataset yang ingin ditampilkan adalah buku yang ditulis setelah tahun 1950 dan harganya lebih dari $10. Filter dapat dilakukan dengan menuliskan kode berikut."
      ],
      "metadata": {
        "id": "si_7o6eNNn8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered = df.filter(\"year_written > 1950 AND price > 10 AND title IS NOT NULL\")\n",
        "df_filtered.select(\"title\", \"price\", \"year_written\").show(50, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOWojMRZNrHM",
        "outputId": "0257dbb1-3033-41dd-b14a-c7df5cda8e97"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------+-----+------------+\n",
            "|title                        |price|year_written|\n",
            "+-----------------------------+-----+------------+\n",
            "|The Hours                    |12.35|1999        |\n",
            "|Harry Potter                 |19.95|2000        |\n",
            "|One Hundred Years of Solitude|14.0 |1967        |\n",
            "+-----------------------------+-----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Penggunaan PySpark SQL functions\n",
        "  \n",
        "  PySpark mempunyai fungsi SQL lainnya, misalnya max, aggregate function (groupBy, sum, count dll). Contoh: menampilkan data buku paling mahal, dapat menggunakan fungsi max."
      ],
      "metadata": {
        "id": "WpJ7bMBSNwX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as f\n",
        "maxValue = df_filtered.agg(f.max(\"price\")).collect()[0][0]\n",
        "print(\"maxValue: \",maxValue)\n",
        "df_filtered.select(\"title\",\"price\").filter(df.price == maxValue).show(20, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEjXZFMiN1we",
        "outputId": "de5a136c-3aca-46ce-b0aa-708ea69ed49a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "maxValue:  19.95\n",
            "+------------+-----+\n",
            "|title       |price|\n",
            "+------------+-----+\n",
            "|Harry Potter|19.95|\n",
            "+------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tugas"
      ],
      "metadata": {
        "id": "KqXQWwG2N7Og"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Tampilkan data buku dengan harga paling murah!"
      ],
      "metadata": {
        "id": "r4_mosb1OCuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(\"price=\"+str(df.agg(f.min(\"price\")).collect()[0][0])).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JdzTvvbOGNz",
        "outputId": "403a7937-ca01-4a3d-cdf1-6ba8d6162f99"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+------------+-----+-----------+------------+\n",
            "|          author|     edition|price|      title|year_written|\n",
            "+----------------+------------+-----+-----------+------------+\n",
            "|Dickens, Charles|Random House| 5.75|Bleak House|        1870|\n",
            "+----------------+------------+-----+-----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Tampilkan jumlah terbit buku dikategorikan setiap tahun ditulis!"
      ],
      "metadata": {
        "id": "0TalogI7OTfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy('year_written').count().sort(df.year_written).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afBJfEawOVv3",
        "outputId": "5b207630-c2d6-4e5c-c00d-f3438a2a024d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----+\n",
            "|year_written|count|\n",
            "+------------+-----+\n",
            "|        1603|    1|\n",
            "|        1814|    1|\n",
            "|        1862|    1|\n",
            "|        1865|    2|\n",
            "|        1870|    1|\n",
            "|        1875|    1|\n",
            "|        1922|    1|\n",
            "|        1925|    1|\n",
            "|        1937|    1|\n",
            "|        1967|    1|\n",
            "|        1999|    1|\n",
            "|        2000|    1|\n",
            "+------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Tampilkan data buku termahal tiap tahun penulisannya!"
      ],
      "metadata": {
        "id": "G9igvWm9OY7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy('year_written').max(\"price\").sort(df.year_written).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DF-o926BObNi",
        "outputId": "6bece09b-5b17-4445-a042-b96b5cec8edd"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+----------+\n",
            "|year_written|max(price)|\n",
            "+------------+----------+\n",
            "|        1603|      7.95|\n",
            "|        1814|      18.2|\n",
            "|        1862|      7.75|\n",
            "|        1865|      12.7|\n",
            "|        1870|      5.75|\n",
            "|        1875|      13.5|\n",
            "|        1922|      29.0|\n",
            "|        1925|      25.0|\n",
            "|        1937|     27.45|\n",
            "|        1967|      14.0|\n",
            "|        1999|     12.35|\n",
            "|        2000|     19.95|\n",
            "+------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Tampilkan data buku termurah tiap tahun penulisannya!"
      ],
      "metadata": {
        "id": "hjoekmk4OeZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy('year_written').min(\"price\").sort(df.year_written).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xzf2rn3FOgeq",
        "outputId": "b5da932f-6d7f-4753-d382-332d80f68741"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+----------+\n",
            "|year_written|min(price)|\n",
            "+------------+----------+\n",
            "|        1603|      7.95|\n",
            "|        1814|      18.2|\n",
            "|        1862|      7.75|\n",
            "|        1865|      5.76|\n",
            "|        1870|      5.75|\n",
            "|        1875|      13.5|\n",
            "|        1922|      29.0|\n",
            "|        1925|      25.0|\n",
            "|        1937|     27.45|\n",
            "|        1967|      14.0|\n",
            "|        1999|     12.35|\n",
            "|        2000|     19.95|\n",
            "+------------+----------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}